{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Projects' 폴더 'test' 파일 'text' 열에 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: beautifulsoup4\n",
      "Version: 4.9.3\n",
      "Summary: Screen-scraping library\n",
      "Home-page: http://www.crummy.com/software/BeautifulSoup/bs4/\n",
      "Author: Leonard Richardson\n",
      "Author-email: leonardr@segfault.org\n",
      "License: MIT\n",
      "Location: c:\\users\\dsme\\anaconda3\\lib\\site-packages\n",
      "Requires: soupsieve\n",
      "Required-by: conda-build\n"
     ]
    }
   ],
   "source": [
    "!pip show BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords # 텍스트 데이터 전처리를 위해 별도로 NLTK data를 다운로드 (수동 다운로드 - https://lemontia.tistory.com/802)\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2467호선 M#20A 순회 점검 중 발생된 comment 사항 가피스 제거 후 n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2469 10H AIR TEST 구간 취부 BKT M.T, 노치 1개 MT(현장 반...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2469 63B,C,80E damage로 인한 RENEW(TANK TEST 전 RE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2469 B#405 N2 BOX LEAK 부 가우징 후 M.T\\r\\n- LEAK부 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2469호선 진수 전 잔여 곡직 수정작업 요청 건으로 적절한 시점에 조치 요청 드립...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  2467호선 M#20A 순회 점검 중 발생된 comment 사항 가피스 제거 후 n...\n",
       "1  2469 10H AIR TEST 구간 취부 BKT M.T, 노치 1개 MT(현장 반...\n",
       "2  2469 63B,C,80E damage로 인한 RENEW(TANK TEST 전 RE...\n",
       "3  2469 B#405 N2 BOX LEAK 부 가우징 후 M.T\\r\\n- LEAK부 ...\n",
       "4  2469호선 진수 전 잔여 곡직 수정작업 요청 건으로 적절한 시점에 조치 요청 드립..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel('test.xlsx')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textmining(file_name, raw_data):\n",
    "    \n",
    "    train = pd.read_excel(file_name)\n",
    "    \n",
    "    # 1) html 태그 제거\n",
    "    delete_html = BeautifulSoup(raw_data, 'html.parser').get_text()\n",
    "    \n",
    "    # 2) 특수문자 제거\n",
    "    only_letters = re.sub('[^a-zA-Z가-힣]', ' ', delete_html)\n",
    "    \n",
    "    # 3) 모두 소문자로 and 토큰화\n",
    "    lower_case = only_letters.lower().split()\n",
    "    \n",
    "    # 4) 불용어 제거\n",
    "    stops = set(stopwords.words('english')) # 파이썬에서는 리스트보다 세트로 찾는 게 훨씬 빠르다\n",
    "    delete_stopwords = [w for w in lower_case if not w in stops]\n",
    "    \n",
    "    # 5) 포터스테밍\n",
    "    words_stemming = [stemmer.stem(w) for w in delete_stopwords]\n",
    "    \n",
    "    # 6) 분리되어 있는 리스트를 '공백으로 결합된 문자열'로 반환\n",
    "    return(' '.join(words_stemming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터 함수에 적용\n",
    "\n",
    "final_data =[]\n",
    "for i in range(0, train['text'].size):\n",
    "    final_data.append(textmining('test.xlsx', train['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1. CountVectorizer 생성\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word', # 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수 중 어떤 단위로 토큰화 할지를 정의\n",
    "                             tokenizer = None, # 함수 또는 None (디폴트), 기본으로 제공하고 있는 방법 외에 방법으로 토큰화 하고자 할 때 함수를 만들어서 넣어줄 수 있음\n",
    "                             preprocessor = None, # 토큰화 전에 텍스트를 별도로 전처리 할 함수가 있다면 입력\n",
    "                             stop_words = None, \n",
    "                             min_df = 5, # 토큰이 나타날 최소 문서 개수 -> 2번 미만으로 나온 단어(토큰)은 무시\n",
    "                             ngram_range=(1, 3), # 하나의 토큰에 들어가는 단어의 수 결정 1, 3 -> I, I am, I am tired\n",
    "                             max_features = 10000 # 생성되는 최대 토큰수(행렬의 열의 수)\n",
    "                            )\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2. 벡터화 fit 사용\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(final_data)\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3. 생성된 matrix를 DF로 만들기\n",
    "\n",
    "dist = np.sum(train_data_features, axis=0) # 각 feature의 총 빈도 확인 (numpy axis 개념 엑셀 참조)\n",
    "final = pd.DataFrame(dist, columns=vocab) # DF로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_excel('Textmining_result.xlsx')\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
